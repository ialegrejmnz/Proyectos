{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Módulo Deep Learning\n","## Actividad 2: Reinforcement Learning: **Frozen lake problem**"],"metadata":{"id":"3qMnOMrtnwtE"}},{"cell_type":"markdown","source":["- David Moreno Vituri\n","- Ricardo Sánchez Olivares\n","- Íñigo Alegre Jiménez"],"metadata":{"id":"J4d_Yp6jeevx"}},{"cell_type":"markdown","source":["# Actividad Reinforcemente Learning\n","\n","Resolver el problema del Frozen lake de OpenAI Gym. Documentación: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n","\n","## Objetivos\n","- Conseguir movermos aleatoriamente hasta cumplir el objetivo\n","- Conseguir que el agente aprenda con Q-learning\n","- (Opcional) Probar con otros hiperparámetros\n","- (Opcional) Modificar la recompensa\n","\n","## Consideraciones\n","- No hay penalizaciones\n","- Si el agente cae en un \"hole\", entonces done = True y se queda atascado sin poder salir (al igual que ocurre cuando llega al \"goal\")\n","\n","## Normas a seguir\n","\n","- Se debe entregar un **ÚNICO GOOGLE COLAB notebook** (archivo .ipynb) que incluya las instrucciones presentes y su **EJECUCIÓN!!!**.\n","- Poner el nombre del grupo en el nombre del archivo y el nombre de todos los integrantes del grupo al inicio del notebook.\n","\n","## Criterio de evaluación\n","\n","- Seguimiento de las normas establecidas en la actividad.\n","- Corrección en el uso de algoritmos, modelos y formas idiomáticas en Python.\n","- El código debe poder ejecutarse sin modificación alguna en Google Colaboratory."],"metadata":{"id":"_ypTIoCpeiSW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TulMfncIouVp","executionInfo":{"status":"ok","timestamp":1730412757937,"user_tz":-60,"elapsed":20884,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"e57d67b9-6aa2-4b86-bf4c-20a9f6e566e7"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **Instalamos librerías**"],"metadata":{"id":"9Ly604VTn1ue"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXbvm0y9No4N","outputId":"23dca80f-3bda-42ca-bad8-e5997b1d13a7","executionInfo":{"status":"ok","timestamp":1730408683482,"user_tz":-60,"elapsed":5817,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (1.24.4)\n","Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.10/dist-packages (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.13.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.24.4)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n"]}],"source":["!pip install numpy==1.24.4\n","!pip install gym==0.17.3"]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","from time import sleep\n","from IPython.display import clear_output\n","import random as rd"],"metadata":{"id":"-aiKby2RNy-T","executionInfo":{"status":"ok","timestamp":1730408688581,"user_tz":-60,"elapsed":1203,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["##**Definición del entorno**"],"metadata":{"id":"shd3NqyQn9IO"}},{"cell_type":"code","source":["# Definimos el entorno\n","env = gym.make('FrozenLake-v0', desc=None, map_name=\"4x4\", is_slippery=False)"],"metadata":{"id":"1_3z3ZByoAcO","executionInfo":{"status":"ok","timestamp":1730408692547,"user_tz":-60,"elapsed":1760,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Fijamos una semilla\n","seed_value = 42\n","env.seed(seed_value)\n","np.random.seed(seed_value)"],"metadata":{"id":"E_Nw22y00NEH","executionInfo":{"status":"ok","timestamp":1730408695180,"user_tz":-60,"elapsed":198,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["env.reset() # En este caso, empieza desde la misma posición inicial\n","print(env.render())"],"metadata":{"id":"fETKbHsBOGtB","executionInfo":{"status":"ok","timestamp":1730408699861,"user_tz":-60,"elapsed":213,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"457596ed-2691-4f23-e164-7f214d2227e7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","None\n"]}]},{"cell_type":"code","source":["print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0p9Zxwz0UNs","executionInfo":{"status":"ok","timestamp":1730408707298,"user_tz":-60,"elapsed":238,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"5e4a86bc-d64e-4c3c-b409-1fd4d1dd04ae"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Action Space Discrete(4)\n","State Space Discrete(16)\n"]}]},{"cell_type":"markdown","source":["Acciones posibles:\n","* 0: izquierda\n","* 1: abajo\n","* 2: derecha\n","* 3: arriba"],"metadata":{"id":"JarMsz_-0YfL"}},{"cell_type":"code","source":["# Identificador de estado\n","state = env.s\n","print(\"State:\", state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xyya51AU0shk","executionInfo":{"status":"ok","timestamp":1730408730556,"user_tz":-60,"elapsed":208,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"3614117a-bf8d-4560-8f1f-c2ee73aa71c3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 0\n"]}]},{"cell_type":"markdown","source":["## **¡Nos movemos aleatoriamente!**"],"metadata":{"id":"jAHw0ZBm1C1-"}},{"cell_type":"code","source":["steps = 0\n","env.reset()\n","env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itxDrfce3-x0","executionInfo":{"status":"ok","timestamp":1730410017735,"user_tz":-60,"elapsed":219,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"d5a6dfb8-5f18-435f-e728-26ec19c6b658"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"]}]},{"cell_type":"code","source":["# Acciones: 0=izquierda, 1=abajo, 2=derecha, 3=arriba\n","action = 2\n","state, reward, done, info = env.step(action)\n","\n","print(\"State:\", state)\n","print(state, reward, done, info)\n","\n","env.s = state\n","env.render()\n","\n","steps += 1\n","\n","print(f\"Step: {steps}\")\n","\n","# hemos comprobado que efectivamente, cuando caemos en un hueco, no podemos movernos más"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5EoVvD61Gjb","executionInfo":{"status":"ok","timestamp":1730410019926,"user_tz":-60,"elapsed":206,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"79d0861f-c94f-4192-a86f-a3b10901506b"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 1\n","1 0.0 False {'prob': 1.0}\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","Step: 1\n"]}]},{"cell_type":"markdown","source":["Recompensas:\n","* Cada paso: 0\n","* Caerse por el acantilado: 0 pero pone done = True\n","* Llegar al destino: +1 y pone done = True"],"metadata":{"id":"ymh4tQ-0aX_3"}},{"cell_type":"markdown","source":["## Nos movemos aleatoriamente hasta llegar al objetivo"],"metadata":{"id":"Iui6uM2AblSF"}},{"cell_type":"code","source":["done = False  # inicializamos done a False indicando que no se ha terminado\n","\n","env.reset()\n","\n","timestep, caidas, final = 0, 0, 0\n","\n","while not ((done) & (final==1)):\n","\n","  action = env.action_space.sample() # con \"sample\" elegimos una de las acciones del action_space\n","  state, final, done, info = env.step(action) # con \"step\" realizamos la acción elegida\n","\n","  if ((done == True)&(final==0)):\n","      caidas += 1 # sumamos una penalización si el muñeco se cae a un hueco\n","      env.reset() # ponemos el muñeco en el punto de partida porque no se puede mover dentro de un hueco\n","\n","  timestep += 1\n","\n","print(\"Timesteps taken: {}\".format(timestep))\n","print(\"Penalties incurred: {}\".format(caidas))\n","env.render() # visualizamos el estado final"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otO23Jhtbp4t","executionInfo":{"status":"ok","timestamp":1730410026032,"user_tz":-60,"elapsed":208,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"d848b6df-b469-4803-dba4-c63009a0da32"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Timesteps taken: 932\n","Penalties incurred: 128\n","  (Right)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n"]}]},{"cell_type":"code","source":["# vamos a mostrar como consigue llegar hasta la meta de manera aleatoria\n","class bcolors:\n","    RED= '\\u001b[31m'\n","    GREEN= '\\u001b[32m'\n","    RESET= '\\u001b[0m'\n","\n","done = False\n","\n","env.reset()\n","\n","timestep, caidas, final = 0, 0, 0\n","total_reward = 0\n","\n","while not ((done) & (final==1)):\n","  action = env.action_space.sample() # con \"sample\" elegimos una de las acciones del action_space\n","  state, final, done, info = env.step(action) # con \"step\" realizamos la acción elegida\n","\n","  if ((done == True)&(final==0)):\n","      caidas += 1 # sumamos una penalización si el muñeco se cae a un hueco\n","      env.reset() # ponemos el muñeco en el punto de partida porque no se puede mover dentro de un hueco\n","\n","  total_reward += final\n","  timestep += 1\n","\n","  # Print each step\n","  clear_output(wait=True)\n","  env.render()\n","  print(\"\\nIteración: {}\".format(timestep))\n","  print(\"Caidas: {}\".format(caidas))\n","  if final == 0:\n","    print(f\"Recompensa actual: {bcolors.RED}{final}{bcolors.RESET}\")\n","  else:\n","    print(f\"Recompensa actual: {bcolors.GREEN}{final}{bcolors.RESET}\")\n","  print(\"\")\n","  print('Estado actual', state)\n","  sleep(.5)\n","\n","print(\"Pasos dados: {}\".format(timestep))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abtFEuURevjL","executionInfo":{"status":"ok","timestamp":1730411838159,"user_tz":-60,"elapsed":918322,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"4cfb6fb6-c0ce-4ee7-c7ea-8c40a900ca3c"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Right)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","\n","Iteración: 1826\n","\\Caidas: 253\n","Recompensa actual: \u001b[32m1.0\u001b[0m\n","\n","Estado actual 15\n","Pasos dados: 1826\n","Caidas: 253\n"]}]},{"cell_type":"markdown","source":["## Q-Learning entrenamiento"],"metadata":{"id":"d3jR4zxNiJIX"}},{"cell_type":"markdown","source":["Comenzamos definiendo la función epsilon greedy policy, la cual nos permite indicar al modelo cuanto debe explorar y cuanto debe 'explotar' la opción más rentable. Cuanto mayor sea epsilon, más explora el modelo y viceversa."],"metadata":{"id":"COwVfqo-iYiQ"}},{"cell_type":"code","source":["#trade-off entre explorar y explotar\n","def epsilon_greedy_policy(epsilon, q_table, state, env):\n","  if rd.random() < epsilon:\n","      action = env.action_space.sample() #explorar\n","  else:\n","      action = np.argmax(q_table[state]) #explotar\n","  return action"],"metadata":{"id":"B28MwMEiiNQy","executionInfo":{"status":"ok","timestamp":1730412064184,"user_tz":-60,"elapsed":228,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["Ahora pasamos a entrenar y actualizar la Q-table para que más tarde el modelo sepa el camino más efectivo y rápido para alcanzar el objetivo."],"metadata":{"id":"2DvY2tFDj5b7"}},{"cell_type":"code","source":["# Hyperparameters\n","alpha = 0.2 # tasa de aprendizaje\n","gamma = 0.7 # tasa de descuento\n","epsilon = 0.15 # greedy policy\n","\n","# For plotting metrics\n","all_timestep = []\n","all_penalties = []\n","\n","episodes = 100000\n","\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","for i in range(episodes):\n","  state = env.reset()\n","\n","  timestep, caidas, final = 0, 0, 0\n","  done = False\n","\n","  while not ((done) & (final==1)):\n","\n","    # Elegimos y ejecutamos la acción\n","    action = epsilon_greedy_policy(epsilon, q_table, state, env) # aplicamos la greedy policy\n","    next_state, final, done, info = env.step(action) # tomamos la acción elegida\n","\n","    # actualizamos la Q-table\n","    old_value = q_table[state, action] # en la Q-table, tomamos el valor Q de la acción elegida para el estado actual\n","    next_max = np.max(q_table[next_state]) # en la Q-table, tomamos el máximo entre los valores Q para el nuevo estado\n","    new_value = (1 - alpha) * old_value + alpha * (final + gamma * next_max) # actualizamos el valor Q\n","    q_table[state, action] = new_value\n","\n","    if ((done == True)&(final==0)):\n","        caidas += 1 # sumamos una penalización si el muñeco se cae a un hueco\n","        env.reset() # ponemos el muñeco en el punto de partida porque no se puede mover dentro de un hueco\n","\n","    state = next_state\n","\n","  if i % 100 == 0:\n","    clear_output(wait=True)\n","    print(f\"Episodio: {i+1}\")\n","\n","  timestep += 1\n","\n","clear_output(wait=True)\n","print(f\"Episodio: {i+1}\")\n","print(\"¡Entrenamiento finalizado!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xse44watj4R0","executionInfo":{"status":"ok","timestamp":1730412165172,"user_tz":-60,"elapsed":96509,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"6d6c4d8a-3fd9-4284-ccfe-9cbd6fcba815"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Episodio: 100000\n","¡Entrenamiento finalizado!\n"]}]},{"cell_type":"markdown","source":["Una vez hemos actualizado la Q-table ya no necesitamos explorar (el camino más eficaz se encuentra ahí). Es por ello que cambiamos la greedy policy de manera que la acción elegida sea siempre escoger como siguiente movimiento aquel que tiene mayor Q-value."],"metadata":{"id":"Hc6y9gjmmlUi"}},{"cell_type":"code","source":["class bcolors:\n","  RED= '\\u001b[31m'\n","  GREEN= '\\u001b[32m'\n","  RESET= '\\u001b[0m'\n","\n","env.reset()\n","env.render()\n","done = False\n","\n","timestep, caidas, final = 0, 0, 0\n","total_reward = 0\n","\n","while not ((done) & (final==1)):\n","  action = np.argmax(q_table[state])\n","  state, final, done, info = env.step(action) # con \"step\" realizamos la acción elegida\n","\n","  if ((done == True)&(final==0)):\n","    caidas += 1 # sumamos una penalización si el muñeco se cae a un hueco\n","    env.reset() # ponemos el muñeco en el punto de partida porque no se puede mover dentro de un hueco\n","\n","  total_reward += final\n","  timestep += 1\n","\n","  # Print each step\n","  sleep(3)\n","  clear_output(wait=True)\n","  env.render()\n","\n","  if final == 0:\n","    print(f\"Recompensa actual: {bcolors.RED}{final}{bcolors.RESET}\")\n","  else:\n","    print(f\"Recompensa actual: {bcolors.GREEN}{final}{bcolors.RESET}\")\n","  print(\"\")\n","  print('Estado actual', state)\n","  sleep(.5)\n","\n","print(\"Timesteps taken: {}\".format(timestep))\n","print(\"Total de caidas: {}\".format(caidas))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkL2sABZmk_g","executionInfo":{"status":"ok","timestamp":1730412638889,"user_tz":-60,"elapsed":24741,"user":{"displayName":"Íñigo Alegre Jiménez","userId":"13792862175353659258"}},"outputId":"6c34d20c-23c2-452e-d23f-1bf3db5d34fa"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Right)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","Recompensa actual: \u001b[32m1.0\u001b[0m\n","\n","Estado actual 15\n","Timesteps taken: 7\n","Total de caidas: 0\n"]}]}]}